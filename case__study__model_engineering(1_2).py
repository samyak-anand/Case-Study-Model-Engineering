# -*- coding: utf-8 -*-
"""Case _Study_ Model_Engineering(1_2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PUiGK8Fbzi3x9Y3NE-AesryCllClJmtz
"""

# Importing pandas for data manipulation and analysis
import pandas as pd

# Importing numpy for numerical computing
import numpy as np

# Importing io for input/output operations
import io

# Importing classification models for building predictive models
from sklearn.tree import DecisionTreeClassifier  # Decision Tree Classifier
from sklearn.ensemble import RandomForestClassifier  # Random Forest Classifier
from sklearn.linear_model import LogisticRegression  # Logistic Regression Classifier
from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors Classifier
from xgboost import XGBClassifier  # XGBoost Classifier
import xgboost as xgb

# Importing metrics for model evaluation
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, confusion_matrix,
                             classification_report, r2_score, mean_absolute_error,
                             mean_squared_error, roc_curve, auc, precision_recall_curve)

# Importing model selection and hyperparameter tuning utilities
from sklearn.model_selection import (GridSearchCV, train_test_split, learning_curve,
                                     StratifiedKFold, cross_val_score)

# Importing preprocessing utilities for feature scaling
from sklearn.preprocessing import StandardScaler, LabelBinarizer, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.calibration import label_binarize
# Importing visualization libraries
import matplotlib.pyplot as plt  # Matplotlib for basic plotting
import seaborn as sns  # Seaborn for enhanced visualization
import matplotlib.lines as mlines  # For drawing lines in plots
import matplotlib.patches as patches  # For drawing shapes in plots

# Importing library for handling imbalanced datasets
from imblearn.over_sampling import SMOTE  # Synthetic Minority Over-sampling Technique

# Importing SHAP library for model interpretability
import shap  # SHapley Additive exPlanations

# Read the Excel file into a DataFrame
df = pd.read_excel(r"/home/samyak/Python_Projects/Case _Study_ Model_Engineering/Datasets/PSP_Jan_Feb_2019.xlsx")

# Display the DataFrame
print(df.head())

print(df.columns)

missing_values= df.isnull().sum()

missing_data =pd.DataFrame({
    'Missing Values': missing_values,
    #'Percentage Missing Value': Percentage_missing_value
})
print(f"The dataframe has {len(missing_values)} rows with missing values.")

data_type=df.dtypes
print(f"Data Type Information:\n{data_type}")

#Print the number of rows in the DataFrame
print("Number of rows in ideal data set : ",len(df))

for column in df.columns:
    unique_values= df[column].unique
    print(f"Column: {column}\nUnique Values: {unique_values}\n")
    # Perform mathematical operations on a specific column of data.
    if np.issubdtype(df[column].dtype,np.number):  # Check if the datatype is numeric.
        mean = df[column].mean()                   # Calculate the mean of the numerical column.
        std = df[column].std()                     # Calculate the standard deviation of the numerical column.
        std = df[column].std()                     # Calculate the standard deviation of the numerical column.
        std_dev = df[column].std()                # Calculate the standard deviation of the numerical column.

#Print the number of column in the Data Frame
print("Number of column in ideal data set : ",len(column))

# Displaying the information about the DataFrame, including data type and memory usage
# of each column.
print("Data Frame Information")
print("-----------------------")
print(f"Number of rows: {len(df)}")
print(f"Number of columns: {len(df.columns)}")
for col in df.columns:
    print(f"\nColumn Name: {col}")
    print(f"Data Type: {type(df[col].values[0])}")

print(df.head())

#Dropint hr "Unnamed: 0" column form the Data Frame
df =df.drop('Unnamed: 0', axis=1)
# Print the remaining columns after dropping 'Unnamed: 0'
print(df.head())

print(df['amount'].describe().to_frame().T)

print(df['success'].describe().to_frame().T)

print(df['PSP'].describe().to_frame().T)

print(df['3D_secured'].describe().to_frame().T)

print(df['card'].describe().to_frame().T)

print(df['country'].describe().to_frame().T)

plt.figure(figsize=(10,6))
sns.set_style("whitegrid")
sns.histplot(df['amount'], kde=True, color='red')  # Change the color of the histogram
kde_line = mlines.Line2D([], [], color='blue', linewidth=2, label='KDE')
plt.legend(handles=[kde_line])
plt.title('Distribution of Transaction Amount')
plt.xlabel('Transaction Amount')
plt.ylabel('Frequency')
plt.gca().get_lines()[-1].set_color('blue')  # Change the color of the KDE line
plt.show()

def plot_distribution(column, title, xlabel, ylabel):
    sns.set_style("whitegrid")

    # Print summary statistics for the column
    print(f"\nSummary statistics for '{column}' column:")
    print(df[column].value_counts())
    print("\n")

    # Create a countplot for the column with hue and legend parameters
    ax = sns.countplot(x=column, hue=column, data=df, palette='Reds', legend=False)

    # Create a grid
    plt.grid(True)

    # Set plot title and axis labels
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)

    # Add count labels to the bars
    for p in ax.patches:
        height = p.get_height()
        ax.text(p.get_x() + p.get_width() / 2.,
                height + 3,
                '{:1.0f}'.format(height),
                ha="center")

    # Show the plot distribution
    plt.show()

# Call the function for different columns
plot_distribution('success', 'Distribution of Success', 'Success', 'Count')

plot_distribution('3D_secured', 'Distribution of 3D_secured', '3D_secured', 'Count')

plot_distribution('country', 'Distribution of Country', 'Country', 'Count')

plot_distribution('PSP', 'Distribution of PSP', 'PSP', 'Count')

plot_distribution('card', 'Distribution of Card', 'Card', 'Count')

# Create a boxplot with 'success' on the x-axis and 'amount' on the y-axis
sns.boxplot(x='success', y='amount', data=df, palette='Reds', hue='success')

# Set plot title and axis labels
plt.title('Relationship between Success and Transaction Amount')
plt.xlabel('Success')
plt.ylabel('Transaction Amount')

# Show the plot relationship between success and transaction amount
plt.show()

# Set the default color palette for seaborn to 'Dark'
sns.set_palette('dark')

# Create a pair plot for the selected variables
pair_plot = sns.pairplot(df[['amount', 'success', '3D_secured']])

# Set the overall title for the pair plot
pair_plot.fig.suptitle('Pair Plot of Variables')

# Show the plot
plt.show()

# Create a pair plot for the 'amount', 'success', and '3D_secured' variables
# with hue based on the 'card' variable
sns.pairplot(df, hue='card', vars=['amount', 'success', '3D_secured'],
             palette='Reds')

# Set the overall title for the pair plot
plt.suptitle('Pair Plot of Variables by Card Type')

# Show the plot
plt.show()

# Select relevant columns for correlation analysis
selected_columns = ['amount', 'success', '3D_secured']
# Calculate the correlation matrix
correlation_matrix = df[selected_columns].corr()
# Create a heatmap to visualize the correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='Reds', linewidths=.5)
# Set plot title
plt.title('Correlation Matrix: Amount, Success, and 3D_secured')
# Show the plot
plt.show()

# Create a bar plot to visualize success rate by card type
fig, ax = plt.subplots(figsize=(15, 6))

# Use a bar plot with 'card' on the x-axis, 'success' on the y-axis, and no confidence interval (errorbar=None)
barplot = sns.barplot(x='card', y='success', hue='card', data=df, dodge=True, palette='Reds', ax=ax, legend=False)

# Set plot title and axis labels
plt.title('Success Rate by Card Type')
plt.xlabel('Card Type')
plt.ylabel('Success Rate')

# Rotate x-axis labels to prevent overlapping
plt.xticks(rotation=45)

# Add labels to the bars
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width() / 2.,
            height + 0.01,  # Adjust this value for vertical position of the label
            f'{height:.2f}',  # Format the label as needed
            ha="center")

# Show the plot
plt.show()

# Convert 'tmsp' column to datetime format
df['tmsp'] = pd.to_datetime(df['tmsp'])

# Set 'tmsp' as the index of the DataFrame
df.set_index('tmsp', inplace=True)

# Resample the data by day and plot the daily transaction volume
df.resample('D').size().plot(title='Daily Transaction Volume', figsize=(12, 6), cmap='PuOr')

# Set x-axis and y-axis labels
plt.xlabel('Date')
plt.ylabel('Transaction Volume')

# Show the plot
plt.show()

# Create a boxplot to visualize transaction amounts by country
plt.figure(figsize=(14, 8))
# Use a boxplot with 'country' on the x-axis and 'amount' on the y-axis
sns.boxplot(x='country', y='amount', data=df, palette='Reds', hue='country')
# Set plot title, axis labels, and rotate x-axis labels for better readability
plt.title('Boxplot of Transaction Amounts by Country')
plt.xlabel('Country')
plt.ylabel('Transaction Amount')
plt.xticks(rotation=45)
# Show the plot
plt.show()

# Resample the DataFrame by day and calculate the sum for each day
df_resampled = df.resample('D').sum()
# Create a line plot for success and failure over time
plt.figure(figsize=(12, 6))
# Use lineplot to plot 'success' and 'failure' against the resampled dates
sns.lineplot(x=df_resampled.index, y='success', data=df_resampled, label='Success')
sns.lineplot(x=df_resampled.index, y=1 - df_resampled['success'], label='Failure')
# Set plot title, x-axis label, y-axis label, and add legend
plt.title('Time Series of Success and Failure')
plt.xlabel('Date')
plt.ylabel('Count')
plt.legend()
# Show the plot
plt.show()

# Create a figure
plt.figure(figsize=(12, 6))
# Calculate the daily success rate and plot it over time
success_rate_by_date = df.resample('D')['success'].mean()
# Use a line plot with markers to visualize the success rate over time
success_rate_by_date.plot(marker='o', linestyle='-', color='color'[0])
# Set plot title, x-axis label, y-axis label
plt.title('Success Rate Over Time')
plt.xlabel('Date')
plt.ylabel('Success Rate')
# Show the plot
plt.show()

def process_transaction_fees(df):
    # Reset the index of the DataFrame
    df.reset_index(inplace=True)

    # Define dictionaries for mapping success and failure transaction fees based on card type
    success_fee_mapping = {
        'Moneycard': 5,
        'Goldcard': 10,
        'UK_Card': 3,
        'Simplecard': 1
    }
    failed_fee_mapping = {
        'Moneycard': 2,
        'Goldcard': 5,
        'UK_Card': 1,
        'Simplecard': 0.5
    }

    # Create a new column 'transaction_fee' based on success or failure, using np.where
    df['transaction_fee'] = np.where(df['success'] == 1, df['PSP'].map(success_fee_mapping),
                                      df['PSP'].map(failed_fee_mapping))

    # Print the first few rows of the updated DataFrame
    print("Associated the service fees of Payment Service Providers (PSPs) with the dataset under the column identifier 'transaction_fee' \n")
    print(df.head())

# Call the function with your DataFrame
process_transaction_fees(df)

# Convert 'tmsp' column to datetime format
df['tmsp'] = pd.to_datetime(df['tmsp'])
# Add 'day_of_week' column with numerical mapping (Monday 0, ..., Sunday 6)
df['day_of_week'] = df['tmsp'].dt.dayofweek
# Add 'minute_of_day' column
df['minute_of_day'] = df['tmsp'].dt.hour * 60 + df['tmsp'].dt.minute
# Print the first few rows of the updated DataFrame
print(df.head())

# Sort the DataFrame by 'country', 'amount', 'day_of_week', 'minute_of_day'
df.sort_values(by=['country', 'amount', 'minute_of_day'], inplace=True)
# Create a new column 'payment_attempts' and initialize it with 1
df['payment_attempts'] = 1
# Identify rows where consecutive attempts have the same 'country', 'amount', 'day_of_week',and 'minute_of_day'
# Increment the 'payment_attempts' for those rows
df['payment_attempts'] = df.groupby(['country', 'amount', 'minute_of_day']).cumcount() + 1
# Reset the DataFrame index
df.reset_index(drop=True, inplace=True)
# Display the updated DataFrame
print("Dataset Post Data Transformation \n")

print(df.head())

# Drop the 'tmsp' column
df.drop('tmsp', axis=1, inplace=True)
print("Updated dataset after feature selection")
print(df.head())

def visualize_data(df):
    # Select numeric columns for summary statistics
    numeric_columns = ['transaction_fee']

    # Calculate descriptive statistics for numeric columns
    numeric_summary = df[numeric_columns].describe()

    # Transpose the summary for better readability
    transposed_summary = numeric_summary.transpose()

    # Print the transposed summary
    print("\n", transposed_summary, "\n")

    # Create a histogram to visualize the distribution of transaction fees
    plt.hist(df['transaction_fee'], bins=10, edgecolor='black', color=sns.color_palette("coolwarm_r", 20)[0],
             alpha=0.5)

    # Set plot title, x-axis label, y-axis label
    plt.title('Transaction Fee Histogram')
    plt.xlabel('Transaction Fee')
    plt.ylabel('Frequency')

    # Show the plot
    plt.show()

    # Create a histogram to visualize the distribution of 'day_of_week'
    plt.hist(df['day_of_week'], bins=7, edgecolor='black', color=sns.color_palette("coolwarm_r", 20)[0], alpha=0.5)

    # Set x-axis label, y-axis label, and plot title
    plt.xlabel('Day of Week')
    plt.ylabel('Frequency')
    plt.title('Distribution of Day of Week')

    # Show the plot
    plt.show()

    # Create a histogram to visualize the distribution of 'minute_of_day'
    plt.hist(df['minute_of_day'], bins=24, edgecolor='black', color=sns.color_palette("coolwarm_r", 20)[0],
             alpha=0.5)

    # Set x-axis label, y-axis label, and plot title
    plt.xlabel('Minute of Day')
    plt.ylabel('Frequency')
    plt.title('Distribution of Minute of Day')

    # Show the plot
    plt.show()

    # Count the occurrences of each value in the 'payment_attempts' column and sort by index
    attempt_counts = df['payment_attempts'].value_counts().sort_index()

    # Print the count of payment attempts for each value
    print(attempt_counts)

    # Create a histogram to visualize the distribution of 'payment_attempts'
    plt.hist(df['payment_attempts'], bins=7, edgecolor='black', color=sns.color_palette("coolwarm_r", 20)[0],
             alpha=0.5)

    # Set x-axis label, y-axis label, and plot title
    plt.xlabel('Payment Attempts')
    plt.ylabel('Frequency')
    plt.title('Distribution of Payment Attempts')

    # Show the plot
    plt.show()

# Call the function with your DataFrame
visualize_data(df)

# Examine unique values in 'country' column
unique_countries = df['country'].unique()
print("Unique values in 'country' column:", unique_countries)
# Examine unique values in 'PSP' column
unique_psps = df['PSP'].unique()
print("Unique values in 'PSP' column:", unique_psps)
# Examine unique values in 'card' column
unique_cards = df['card'].unique()
print("Unique values in 'card' column:", unique_cards)

sns.set(style="whitegrid")

# Set the palette to 'Reds'
palette='Reds'
sns.set_palette(palette)

# Box plot for 'amount'
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['amount'] )
plt.title('Box Plot for Amount')
plt.xlabel('Amount')
plt.ylabel('Value')
plt.show()
# Box plot for 'transaction_fee'
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['transaction_fee'])
plt.title('Box Plot for Transaction Fee')
plt.xlabel('Transaction Fee')
plt.ylabel('Value')
plt.show()

# Set a threshold for identifying rare categories
threshold = 10
# Identify rare categories for 'country', 'PSP', and 'card'
rare_country = df['country'].value_counts()[df['country'].value_counts() < threshold].index
rare_PSP = df['PSP'].value_counts()[df['PSP'].value_counts() < threshold].index
rare_card = df['card'].value_counts()[df['card'].value_counts() < threshold].index
# Print the rare categories
print("Rare countries:", rare_country)
print("Rare PSPs:", rare_PSP)
print("Rare cards:", rare_card)

# Identify non-numeric columns
non_numeric_columns = df.select_dtypes(exclude='number').columns

# Drop non-numeric columns before calculating correlation
correlation_matrix = df.drop(columns=non_numeric_columns).corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='Reds', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix')
plt.show()

"""# Developing Model"""

#Developing Model
# Create a copy of the DataFrame
df_1 = df.copy()
# Drop the 'transaction_fee' column from the copied DataFrame
#df_1 = df_1.drop(['PSP'], axis=1)
df_1 = df_1.drop('transaction_fee', axis=1)
# Print the modified DataFrame without the 'transaction_fee' column
print(df_1)
df_encoded_1 = pd.get_dummies(df_1, columns=['country', 'card'])

"""Baseline Model


The baseline model is a simple linear regression model. It assumes that the relationship between the dependent variable and independent variables can be described by a
The baseline model is a simple linear regression model with no interaction terms.
"""

def evaluate_model(model, X_scaled, y, set_name):
    """
    Evaluate the performance of a model on a given dataset.

    Parameters:
    model (object): A trained model object.
    X_scaled (array): The scaled features of the dataset.
    y (array): The target variable of the dataset.
    set_name (str): The name of the dataset (e.g., 'Training', 'Testing').

    Returns:
    None
    """

    # Make predictions using the model
    y_pred = model.predict(X_scaled)

    # Get the predicted probabilities for each class
    y_pred_proba = model.predict_proba(X_scaled)[:, 1]

    # Calculate the precision, recall, F1-score, and AUC-ROC
    precision = precision_score(y, y_pred)
    recall = recall_score(y, y_pred)
    f1 = f1_score(y, y_pred)
    roc_auc = roc_auc_score(y, y_pred)

    # Calculate the accura
    accuracy = accuracy_score(y, y_pred)

    # Get the confusion matrix
    conf_matrix = confusion_matrix(y, y_pred)

    # Print the model performance metrics
    print(f'\n{"="*20} {type(model).__name__} {"="*20}\n')
    print(f'Model Performance on {set_name} set - {type(model).__name__}:')
    print('\n')
    print(f'Accuracy: {accuracy:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1-Score: {f1:.4f}')
    print(f'AUC-ROC: {roc_auc:.4f}')
    print(f'\n{"="*20}{"="*20}\n')
    print(classification_report(y, y_pred))

     # Calculate and print confusion matrix
    cm = confusion_matrix(y, y_pred)
    cm_df1 = pd.DataFrame(cm, index=model.classes_, columns=model.classes_)
    print(f'\n{"="*20}{"="*20}\n')
    print("Confusion Matrix:")
    print(cm_df1)
    plt.figure()
    plt.imshow(cm_df1, cmap='Reds')
    plt.title(f'Confusion Matrix: {set_name} - {type(model).__name__}')
    plt.colorbar()
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tick_params(axis='both', which='major', labelsize=8)
    plt.tight_layout()
    plt.show()

    # Plot the ROC curve
    fpr, tpr, _ = roc_curve(y, y_pred)
    plt.figure(figsize=(7, 5))
    plt.plot([0, 1], [0, 1], 'k--')
    plt.plot(fpr, tpr, label='AUC = %0.4f' % roc_auc)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.show()

# Encode categorical variables using one-hot encoding
df_1_encoded = pd.get_dummies(df_1, columns=['country', 'PSP', 'card'])
# Split the dataset into train, validation, and test sets
#X_original = df_1_encoded.drop('success', axis=1)
X_original = df_1_encoded.drop('success', axis=1)
y_original = df_1_encoded['success']
X_train_original, X_temp_original, y_train_original, y_temp_original = train_test_split(X_original,y_original, test_size=0.2, random_state=42)
X_validation_original, X_test_original, y_validation_original, y_test_original =train_test_split(X_temp_original, y_temp_original, test_size=0.5, random_state=42)

# Create and train a baseline KNN model
baseline_knn_model = KNeighborsClassifier()
baseline_knn_model.fit(X_train_original, y_train_original)


# Evaluate the model on the validation set
evaluate_model(baseline_knn_model, X_validation_original, y_validation_original, "validation set")
# Evaluate the model on the test set
evaluate_model(baseline_knn_model, X_test_original, y_test_original, "test set")

# Encode categorical variables using one-hot encoding
df_1 = pd.get_dummies(df_1, columns=['country', 'PSP', 'card'])
# Print the DataFrame after one-hot encoding
print(df_1)

# Drop the target variable 'success' from the features
X = df_1.drop('success', axis=1)
# Initialize the StandardScaler
scaler = StandardScaler()
# Fit and transform the features using StandardScaler
X_scaled = scaler.fit_transform(X)
# Create a DataFrame with scaled features and include the 'success' column
df_1_scaled = pd.DataFrame(X_scaled, columns=X.columns)
df_1_scaled['success'] = df_1['success']
# Print the DataFrame with scaled features
print(df_1_scaled)
#Extract features and target variable for SMOTE
X_smote = df_1_scaled.drop('success', axis=1)
y_smote = df_1_scaled['success']
# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_smote, y_smote)
# Create a new DataFrame with the resampled features and the target variable
df_1_resampled = pd.DataFrame(X_resampled, columns=X_smote.columns)
df_1_resampled['success'] = y_resampled
# Display the updated DataFrame with SMOTE
print(df_1_resampled)
# Development of Model A
# Split the resampled dataset into features (X_1) and target variable (y_1)
X_1 = df_1_resampled.drop('success', axis=1)
y_1 = df_1_resampled['success']
# Split the dataset into train, validation, and test sets
X_train_1, X_temp, y_train_1, y_temp = train_test_split(X_1, y_1, test_size=0.2, random_state=42)
X_valid_1, X_test_1, y_valid_1, y_test_1 = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
# Initialize a StandardScaler and scale the features for training, validation, and test sets
scaler = StandardScaler()
X_train_scaled_1 = scaler.fit_transform(X_train_1)
X_valid_scaled_1 = scaler.transform(X_valid_1)
X_test_scaled_1 = scaler.transform(X_test_1)

def calculate_classification_metrics(y_true, y_pred, y_pred_proba):
    """
    Calculate various classification metrics and display results.

    Parameters:
    y_true (array): True labels.
    y_pred (array): Predicted labels.
    y_pred_proba (array): Predicted probabilities for positive class.

    Returns:
    dict: Dictionary containing calculated metrics.
    """
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    accuracy = accuracy_score(y_true, y_pred)

    metrics = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'AUC-ROC': roc_auc
    }

    print("Classification Metrics:")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")


    return metrics

def evaluate_model(model, X_scaled, y, set_name):
    """
    Evaluate the performance of a model on a given dataset.

    Parameters:
    model (object): A trained model object.
    X_scaled (array): The scaled features of the dataset.
    y (array): The target variable of the dataset.
    set_name (str): The name of the dataset (e.g., 'Training', 'Testing').

    Returns:
    None
    """
    print(f'Model Performance on {set_name} set - {type(model).__name__}:')
    print("\n")
    y_pred = model.predict(X_scaled)
    y_pred_proba = model.predict_proba(X_scaled)[:, 1]
    calculate_classification_metrics(y, y_pred, y_pred_proba)
    conf_matrix = confusion_matrix(y, y_pred)
    cm_df = pd.DataFrame(conf_matrix, index=model.classes_, columns=model.classes_)

    print(f'\n{"="*20}{"="*20}\n')
    print(f'Classification Report: {set_name} set - {type(model).__name__}  \n')
    print(classification_report(y, y_pred))
    print(f'\n{"="*20}{"="*20}\n')
    print(f"Confusion Matrix: {set_name} set - {type(model).__name__} ")
    print(cm_df)
    print(f'\n{"="*20}{"="*20}\n')
    plt.figure()
    plt.imshow(cm_df, cmap='Reds')
    plt.title(f'Confusion Matrix: {set_name} set - {type(model).__name__}')
    plt.colorbar()
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.tick_params(axis='both', which='major', labelsize=8)
    plt.tight_layout()
    plt.show()

    fpr, tpr, _ = roc_curve(y, y_pred_proba)
    plt.figure(figsize=(7, 5))
    plt.plot([0, 1], [0, 1], 'k--')
    plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y, y_pred_proba):.4f}')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Receiver Operating Characteristic (ROC)Curve - {set_name} set')
    plt.legend(loc="lower right")
    plt.show()

# Model Training and Evaluation
def train_and_evaluate_model(model, X_train_scaled, y_train, X_valid_scaled, y_valid, X_test_scaled, y_test):
    model.fit(X_train_scaled, y_train)

    print(f'\n{"="*20} {type(model).__name__} {"="*20}\n')

    print("Validation Set Evaluation:")
    evaluate_model(model, X_valid_scaled, y_valid, "Validation")

    print("Test Set Evaluation:")
    evaluate_model(model, X_test_scaled, y_test, "Test")

# Logistic Regression
log_reg_model = LogisticRegression(random_state=42)
train_and_evaluate_model(log_reg_model, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1)

# Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)
train_and_evaluate_model(dt_model, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1)

# Random Forest
rf_model = RandomForestClassifier(random_state=42)
train_and_evaluate_model(rf_model, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1)

# XGBoost
xgb_model = xgb.XGBClassifier(random_state=42, objective='multi:softmax', num_class=9)
train_and_evaluate_model(xgb_model, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1)

def optimize_and_evaluate_model(model, param_grid, X_train, y_train, X_valid, y_valid, X_test, y_test, set_name):
    # Initialize GridSearchCV with the given model, hyperparameters, scoring metric, and cross-validation
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)

    # Fit GridSearchCV to the training data
    grid_search.fit(X_train, y_train)

    # Print the best parameters and best score found by GridSearchCV
    print(f'Best Parameters found for {set_name} set - {type(model).__name__}:')
    print(f'{"="*20}{"="*20}')
    print(f"Best Parameters for {set_name} set: {grid_search.best_params_}")
    print(f"Best Score for {set_name} set: {grid_search.best_score_}")
    # Get the best model (i.e., the model with the best hyperparameters)
    best_model = grid_search.best_estimator_

# Perform grid search for Decision Tree model

# Define the grid of hyperparameters for the Decision Tree model
param_grid_dt = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Instantiate the Decision Tree model with a random state for reproducibility
dt_model_A = DecisionTreeClassifier(random_state=42)

# Optimize and evaluate the Decision Tree model using the defined grid of hyperparameters
optimize_and_evaluate_model(dt_model_A, param_grid_dt, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1, set_name="Decision Tree")
train_and_evaluate_model(dt_model_A, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1)

# Perform grid search for Random Forest model

# Define the grid of hyperparameters for the Random Forest model
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Instantiate the Random Forest model with a random state for reproducibility
best_rf_model = RandomForestClassifier(random_state=42)

# Optimize and evaluate the Random Forest model using the defined grid of hyperparameters
optimize_and_evaluate_model(best_rf_model, param_grid_rf, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1, set_name="Random Forest")
train_and_evaluate_model(best_rf_model, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1)

# Define parameter grid for XGBoost model
param_grid_xgb = {
    'max_depth': [3, 4, 5],
    'learning_rate': [0.1, 0.01, 0.001],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Initialize XGBoost model
xgb_model = xgb.XGBClassifier(random_state=42, objective='multi:softmax', num_class=9)

# Perform grid search and evaluate model
optimize_and_evaluate_model(xgb_model, param_grid_xgb, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1, set_name="XGBoost")
train_and_evaluate_model(xgb_model, X_train_scaled_1, y_train_1, X_valid_scaled_1, y_valid_1, X_test_scaled_1, y_test_1)

# Initialize the JS visualization for SHAP plots
shap.initjs()

# Train an XGBoost Regressor model using the validation set
model = xgb.XGBClassifier().fit(X_valid_scaled_1, y_valid_1)

# Create a SHAP explainer for the XGBoost model
explainer = shap.Explainer(model)

# Generate SHAP values for the entire dataset
shap_values = explainer(X)

# visualize the first prediction's explanation
shap.plots.waterfall(shap_values[0])

# visualize the first prediction's explanation with a force plot
shap.plots.force(shap_values[0])

# visualize the Secondary prediction's explanation with a force plot
shap.plots.force(shap_values[1])

# visualize all the training set predictions
shap.plots.force(shap_values[:500])

# summarize the effects of all the features
shap.plots.beeswarm(shap_values)

# create a dependence scatter plot to show the effect of a single feature across the whole dataset
shap.plots.scatter(shap_values[:, "amount"], color=shap_values)

shap.plots.bar(shap_values)

# Precision-Recall Curve for validation and test set of Model 1
# Predict probabilities on the validation set
y_valid_pred_proba = best_rf_model.predict_proba(X_valid_scaled_1)[:, 1]
# Predict probabilities on the test set
y_test_pred_proba = best_rf_model.predict_proba(X_test_scaled_1)[:, 1]
# Compute precision-recall curve values for validation set
precision_valid, recall_valid, thresholds_valid = precision_recall_curve(y_valid_1,
y_valid_pred_proba)
# Compute area under the curve (AUC) for validation set
pr_auc_valid = auc(recall_valid, precision_valid)
# Plot the precision-recall curve for validation set
plt.figure(figsize=(8, 6))
plt.plot(recall_valid, precision_valid, label=f'Validation Set (AUC = {pr_auc_valid:.2f})', color='b')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Validation Set')
plt.legend(loc='lower left')
plt.show()

# Compute precision-recall curve values for test set
precision_test, recall_test, thresholds_test = precision_recall_curve(y_test_1,
y_test_pred_proba)
# Compute area under the curve (AUC) for test set
pr_auc_test = auc(recall_test, precision_test)
# Plot the precision-recall curve for test set
plt.figure(figsize=(8, 6))
plt.plot(recall_test, precision_test, label=f'Test Set (AUC = {pr_auc_test:.2f})', color='r')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Test Set')
plt.legend(loc='lower left')
plt.show()

def evaluate_model(model, X, y, set_name=""):
    """
    Evaluate the performance of a classification model on a given dataset.
    Parameters:
    - model: The trained classification model.
    - X: The feature matrix of the dataset.
    - y: The true labels of the dataset.
    - set_name: The name of the dataset (e.g., "Train", "Validation", "Test").
    Prints:
    - Accuracy, Precision, Recall, F1 Score, ROC AUC Score, Confusion Matrix.
    """
    # Make predictions
    y_pred = model.predict(X)
    # Convert y to 1D array if it's a DataFrame
    if hasattr(y, 'values'):
        y = y.values.ravel()

    # Calculate and print relevant evaluation metrics
    accuracy = accuracy_score(y, y_pred)
    precision = precision_score(y, y_pred, average='weighted')
    recall = recall_score(y, y_pred, average='weighted')
    f1 = f1_score(y, y_pred, average='weighted')
    roc_auc = roc_auc_score(y, y_pred)
    conf_matrix = confusion_matrix(y, y_pred)

    print(f'\n{"="*20} {type(model).__name__} {"="*20}\n')
    print(f'{set_name} Set Evaluation: {type(model).__name__}')
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    cm_df = pd.DataFrame(conf_matrix, index=model.classes_, columns=model.classes_)

    print(f'{set_name} Set Evaluation: {type(model).__name__}')
    print(f'\n{"="*20}{"="*20}\n')
    print(f'Classification Report: {set_name} set - {type(model).__name__}  \n')
    print (classification_report(y,y_pred))
    print(f'\n{"="*20}{"="*20}\n')
    print(f'Confusion Matrix: {set_name} set - {type(model).__name__}  \n')
    print("Confusion Matrix:")

    print(cm_df)
    print(f'\n{"="*20}{"="*20}\n')
    plt.figure()
    plt.imshow(cm_df, cmap='Reds')
    plt.title(f'Confusion Matrix {set_name} Set Evaluation: {type(model).__name__}')
    plt.colorbar()
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.tick_params(axis='both', which='major', labelsize=8)
    plt.tight_layout()
    plt.show()

def plot_roc_curve(fpr, tpr, auc_score, set_name="", model_name=""):
    """
    Plot ROC Curve for a given set.
    Parameters:
    - fpr: False Positive Rate.
    - tpr: True Positive Rate.
    - auc_score: Area Under the ROC Curve.
    - set_name: The name of the dataset (e.g., "Validation", "Test").
    - model_name: The name of the classification model.
    """
    plt.figure(figsize=(8, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{set_name} ROC Curve (AUC = {auc_score:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {set_name} Set: {model_name}')
    plt.legend()
    plt.show()

# Fit the Random Forest model
best_rf_model.fit(X_train_scaled_1, y_train_1)

# Predict probabilities on the validation set
y_valid_pred_proba = best_rf_model.predict_proba(X_valid_scaled_1)[:, 1]

# Calculate precision and recall for different thresholds
precision, recall, thresholds = precision_recall_curve(y_valid_1, y_valid_pred_proba)

# Find the threshold for a desired trade-off (e.g., balance between precision and recall)
desired_precision = 0.85
desired_recall = 0.85

# Find the index of the point on the curve closest to the desired trade-off
closest_point_index = np.argmin(np.abs(precision - desired_precision) + np.abs(recall - desired_recall))

# Get the corresponding threshold
desired_threshold = thresholds[closest_point_index]

adjusted_predictions_valid = (y_valid_pred_proba >= desired_threshold).astype(int)
# Adjust the decision threshold for both validation and test sets
adjusted_threshold = 0.445
adjusted_predictions_valid = (y_valid_pred_proba >= adjusted_threshold).astype(int)
adjusted_predictions_test = (best_rf_model.predict_proba(X_test_scaled_1)[:, 1] >= adjusted_threshold).astype(int)

# Compute ROC Curve for both validation and test sets
fpr_valid, tpr_valid, _ = roc_curve(y_valid_1, y_valid_pred_proba)
roc_auc_valid = auc(fpr_valid, tpr_valid)

fpr_test, tpr_test, _ = roc_curve(y_test_1, best_rf_model.predict_proba(X_test_scaled_1)[:, 1])
roc_auc_test = auc(fpr_test, tpr_test)

# Print the desired threshold
print(f"Desired Threshold: {desired_threshold:.4f}")
# Adjust the decision threshold for the validation set
# Evaluate the model on the validation set
evaluate_model(best_rf_model, X_valid_scaled_1, y_valid_1, set_name="Validation")
plot_roc_curve(fpr_valid, tpr_valid, roc_auc_valid, set_name="Validation", model_name="Random Forest")
# Evaluate the model on the test set
evaluate_model(best_rf_model, X_test_scaled_1, y_test_1, set_name="Test")
plot_roc_curve(fpr_test, tpr_test, roc_auc_test, set_name="Test", model_name="Random Forest")

def plot_confusion_matrix(confusion_matrix, title):
    # Calculate total correctly classified and misclassified instances
    correctly_classified = np.trace(confusion_matrix)
    misclassified = np.sum(confusion_matrix) - correctly_classified

    # Create a new figure and a set of subplots
    fig, ax = plt.subplots()

    # Create a pie chart
    ax.pie([correctly_classified, misclassified], labels=['Correctly Classified', 'Misclassified'], colors=['#ff9999', '#66b3ff'], autopct='%1.1f%%', shadow=True, startangle=140)

    # Display the pie chart
    plt.title(title)
    plt.show()

    # Print the results
    print("Total Correctly Classified Instances:", correctly_classified)
    print("Total Misclassified Instances:", misclassified)

# Confusion matrix of Model A on Validation Set
confusion_matrix_a_validation = np.array([[3259,   786],
                                          [779,  3212]])

plot_confusion_matrix(confusion_matrix_a_validation, 'Correctly vs Misclassified Instances on Validation Set of Model')

# Confusion matrix of Model A on Validation Set
confusion_matrix_a_test = np.array([[3266,  757],
                                    [ 823, 3191]])

plot_confusion_matrix(confusion_matrix_a_test, 'Correctly vs Misclassified Instances on Test Set of Model')

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, 30],
    'min_child_weight': [2, 5, 10],
    'learning_rate': [0.1, 0.01, 0.001]
}

# Create a scaler object
scaler = StandardScaler()

# Fit the scaler on the training data
scaler.fit(X_train_1)

# Scale the training and testing data
X_train_scaled_1 = scaler.transform(X_train_1)
X_test_scaled_1 = scaler.transform(X_test_1)

# Create GridSearchCV
grid_search = GridSearchCV(estimator=XGBClassifier(), param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)

# Fit the model on the training data
grid_search.fit(X_train_scaled_1, y_train_1)

# Access cross-validation results
cv_results_xgb = grid_search.cv_results_
# Print performance metrics for each fold
for i in range(grid_search.n_splits_):
  print(f"\nResults for Fold {i + 1}:")
  print(f"Mean Test Score: {cv_results_xgb[f'mean_test_score'][i]}")
  print(f"Standard Deviation Test Score: {cv_results_xgb[f'std_test_score'][i]}")
  print(f"Params: {cv_results_xgb['params'][i]}")

#Evaluate Cross-Validation Stability of Model A
# Define the parameter grid for hyperparameter tuning
param_grid = {
'n_estimators': [50, 100, 200],
'max_depth': [10, 20, 30],
'min_samples_split': [2, 5, 10],
'min_samples_leaf': [1, 2, 4]
}
# Create GridSearchCV
grid_search = GridSearchCV(estimator=best_rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
# Fit the model on the training data
grid_search.fit(X_train_scaled_1, y_train_1)
# Access cross-validation results
cv_results = grid_search.cv_results_

# Print performance metrics for each fold
for i in range(grid_search.n_splits_):
  print(f"\nResults for Fold {i + 1}:")
  print(f"Mean Test Score: {cv_results[f'mean_test_score'][i]}")
  print(f"Standard Deviation Test Score: {cv_results[f'std_test_score'][i]}")
  print(f"Params: {cv_results['params'][i]}")

# Overfitting Analysis for Model
# Function to plot learning curve
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):
  plt.figure()
  plt.title(title)

  if ylim is not None:
    plt.ylim(*ylim)

  plt.xlabel("Training examples")
  plt.ylabel("F1 Score")

  train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='f1_macro')
  train_scores_mean = np.mean(train_scores, axis=1)
  train_scores_std = np.std(train_scores, axis=1)
  test_scores_mean = np.mean(test_scores, axis=1)
  test_scores_std = np.std(test_scores, axis=1)
  plt.grid()

  plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1,  color="r")
  plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color="g")
  plt.plot(train_sizes, train_scores_mean, 'o-', color="r",label="Training score")
  plt.plot(train_sizes, test_scores_mean, 'o-', color="g",label="Cross-validation score")
  plt.legend(loc="best")

  # Print calculations
  print(f"Train Sizes: {train_sizes}")
  print(f"Training Scores Mean: {train_scores_mean}")
  print(f"Training Scores Std: {train_scores_std}")
  print(f"Test Scores Mean: {test_scores_mean}")
  print(f"Test Scores Std: {test_scores_std}")
  return plt

# Plot learning curve
title = "Learning Curve (Random Forest) for Model"
cv = 5 # Cross-validation folds
plot_learning_curve(best_rf_model, title, X_train_scaled_1, y_train_1, cv=cv)
plt.show()

# Drop 'success' and 'transaction_fee' columns from the original DataFrame
X_original = df.drop(['success', 'transaction_fee'], axis=1)
# One-hot encode categorical columns: 'country', 'PSP', 'card'
X_original_encoded = pd.get_dummies(X_original, columns=['country', 'PSP', 'card'])
# Identify additional columns in X_original_encoded not present in X_train_1
additional_columns = set(X_original_encoded.columns) - set(X_train_1.columns)
if additional_columns:
  print(f"Additional columns in X_original_encoded: {additional_columns}")
# Identify missing columns in X_original_encoded compared to X_train_1
missing_columns = set(X_train_1.columns) - set(X_original_encoded.columns)
if missing_columns:
  print(f"Missing columns in X_original_encoded: {missing_columns}")
# Keep only columns present in X_train_1 in X_original_encoded
X_original_encoded = X_original_encoded[X_train_1.columns]
# Scale the features using the previously defined 'scaler'
X_original_scaled = scaler.transform(X_original_encoded)
# Predict success probabilities using the trained random forest Model A
success_probabilities = xgb_model.predict_proba(X_original_scaled)[:, 1]
# Add the success probabilities as a new column to the original DataFrame
df['success_probabilities'] = success_probabilities
# Display the updated DataFrame
print(df)

# Create a copy of the DataFrame
df_2 = df.copy()
# Drop the 'transaction_fee' column from the copied DataFrame
# Encode categorical variables using one-hot encoding
df_encoded_2 = pd.get_dummies(df_2, columns=['country', 'card'])
# Print the DataFrame after one-hot encoding
print(df_encoded_2)

# Split the dataset into features (X) and target variable (y)
X = df_encoded_2.drop(['PSP', 'success'], axis=1)
y = df_encoded_2['PSP']
# Split the dataset into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
# Create and train a KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Drop the target variables 'PSP' and 'success' from the features
X_2 = df_encoded_2.drop(['PSP', 'success'], axis=1)
# Initialize the StandardScaler
scaler = StandardScaler()
# Fit and transform the features using StandardScaler
X_scaled_2 = scaler.fit_transform(X_2)
# Create a DataFrame with scaled features and include the 'PSP' column
df_scaled_2 = pd.DataFrame(X_scaled_2, columns=X_2.columns)
df_scaled_2[['PSP']] = df_encoded_2[['PSP']]
# Print the DataFrame with scaled features
print(df_scaled_2)

# Split the scaled DataFrame into features (X_smote_2) and target variable (y_smote_2)
X_smote_2 = df_scaled_2.drop('PSP', axis=1)
y_smote_2 = df_scaled_2['PSP']
# Initialize the SMOTE with a random state
smote_2 = SMOTE(random_state=42)
# Resample the dataset using SMOTE
X_resampled_2, y_resampled_2 = smote_2.fit_resample(X_smote_2, y_smote_2)
# Create a DataFrame with resampled features and include the 'PSP' column
df_2_resampled = pd.DataFrame(X_resampled_2, columns=X_smote_2.columns)
df_2_resampled['PSP'] = y_resampled_2
# Print the resampled DataFrame
print(df_2_resampled)

# Split the resampled DataFrame into features (X_2) and target variable (y_2)
X_2 = df_2_resampled.drop('PSP', axis=1)
y_2 = df_2_resampled['PSP']
# Split the dataset into train, validation, and test sets
X_train_2, X_temp_2, y_train_2, y_temp_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)
X_valid_2, X_test_2, y_valid_2, y_test_2 = train_test_split(X_temp_2, y_temp_2, test_size=0.5,random_state=42)
# Normalize Data
scaler = StandardScaler()
X_train_scaled_2 = scaler.fit_transform(X_train_2)
X_valid_scaled_2 = scaler.transform(X_valid_2)
X_test_scaled_2 = scaler.transform(X_test_2)

def evaluate_model2(model, X, y, set_name):
    # Make predictions on the data
    y_pred = model.predict(X)
    # Extract the values of the target variable
    y_values = y.values.ravel() # Convert DataFrame to 1D array
    # Calculate and print relevant evaluation metrics
    precision = precision_score(y_values, y_pred, average='weighted')
    recall = recall_score(y_values, y_pred, average='weighted')
    f1 = f1_score(y_values, y_pred, average='weighted')
    # For binary classification, set multi_class to 'ovr'
    if len(model.classes_) == 2:
        roc_auc = roc_auc_score(y_values, model.predict_proba(X)[:, 1], average='weighted')
    else:
        roc_auc = roc_auc_score(pd.get_dummies(y_values), model.predict_proba(X), average='weighted', multi_class='ovr')


    accuracy = accuracy_score(y_values, y_pred)
    print(f'\n{"="*20} {type(model).__name__} {"="*20}\n')
    print(f'{set_name} Set Evaluation: {type(model).__name__}')
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    print(f"Accuracy: {accuracy:.4f}")
    cm = confusion_matrix(y, y_pred)
    cm_df = pd.DataFrame(cm, index=model.classes_, columns=model.classes_)

    print(f'\n{"="*20}{"="*20}\n')
    print(f'Classification Report: {set_name} set - {type(model).__name__}  \n')
    print(classification_report(y_values, y_pred))
    print (classification_report(y,y_pred))
    print(f'\n{"="*20}{"="*20}\n')
    print(f'Confusion Matrix: {set_name} set - {type(model).__name__}  \n')
    print("Confusion Matrix:")
    print(cm_df)
    plt.figure()
    plt.imshow(cm_df, cmap='Reds')
    plt.title(f'Confusion Matrix {set_name} Set Evaluation: {type(model).__name__}')
    plt.colorbar()
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.tick_params(axis='both', which='major', labelsize=8)
    plt.tight_layout()
    plt.show()

# Create and train Random Forest model with specified parameters


rf_model_B = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=None)

# Perform cross-validation on the training set
cv_scores_rf = cross_val_score(rf_model_B, X_train_scaled_2, y_train_2,
                               scoring='f1_weighted', cv=StratifiedKFold(n_splits=5))

# Print cross-validation scores
print("Cross-Validation Scores:\n", cv_scores_rf)
print(f"Average F1 Weighted Score: {cv_scores_rf.mean():.4f}")

# Fit the Logistic Regression model on the training set
rf_model_B.fit(X_train_scaled_2, y_train_2)
# Print model performance on the validation set
print("Model Performance on Validation set - Random Forest::")
evaluate_model2(rf_model_B, X_valid_scaled_2, y_valid_2, set_name="Validation")
# Print model performance on the test set
print("\nModel Performance on Test set - Random Forest::")
evaluate_model2(rf_model_B, X_test_scaled_2, y_test_2, set_name="Test")

# Random Forest Model B after hyperparameter tuning and cross-validation
# Create a Random Forest model with a set random state
rf_model = RandomForestClassifier(random_state=42)
# Define the parameter grid for hyperparameter tuning
param_grid = {
'n_estimators': [50, 100, 200],
'max_depth': [10, 20, 30]
}

# Use GridSearchCV for hyperparameter tuning
rf_grid = GridSearchCV(rf_model, param_grid, scoring='f1_weighted', cv=StratifiedKFold(n_splits=5), verbose=1)
rf_grid.fit(X_train_scaled_2, y_train_2)
# Get the best model from the grid search
rf_best_model = rf_grid.best_estimator_
# Print the best hyperparameters found
print("Best Hyperparameters:", rf_grid.best_params_)

# Perform cross-validation on the training set with the best model
cv_scores_rf = cross_val_score(rf_best_model, X_train_scaled_2, y_train_2, scoring='f1_weighted', cv=StratifiedKFold(n_splits=5))
# Print cross-validation scores
print("Cross-Validation Scores with Best Model of Random Forest Classification:")
print(cv_scores_rf)
print(f"Average F1 Weighted Score: {cv_scores_rf.mean():.4f}\n")

# Fit the best Random Forest model on the training set
rf_best_model.fit(X_train_scaled_2, y_train_2)

# Print model performance on the validation set
print("Model Performance on Validation set - RandomForestClassifier:")
evaluate_model2(rf_best_model, X_valid_scaled_2, y_valid_2, set_name="Validation")

# Print model performance on the test set
print("Model Performance on Test set - RandomForestClassifier:")
evaluate_model2(rf_best_model, X_test_scaled_2, y_test_2, set_name="Test")

# Precision-Recall curve for validation and test set of Model B
# Binarize the labels for precision-recall curve
from sklearn.preprocessing import StandardScaler, LabelBinarizer

y_valid_2_bin = label_binarize(y_valid_2, classes=np.unique(y_valid_2))
y_test_2_bin = label_binarize(y_test_2, classes=np.unique(y_test_2))
# Predict probabilities on the validation set
y_valid_pred_proba_rf = rf_best_model.predict_proba(X_valid_scaled_2)
# Calculate precision and recall for various thresholds
precision_rf_valid, recall_rf_valid, thresholds_rf_valid = precision_recall_curve(y_valid_2_bin.ravel(), y_valid_pred_proba_rf.ravel())
# Calculate AUC for validation set
auc_rf_valid = auc(recall_rf_valid, precision_rf_valid)
# Plot the precision-recall curve for validation set
plt.figure(figsize=(8, 6))
plt.plot(recall_rf_valid, precision_rf_valid, color='blue', label=f'Random Forest (Validation)AUC: {auc_rf_valid:.4f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - RandomForestClassifier (Validation)')
plt.legend()
plt.grid(True)
plt.show()

# Predict probabilities on the test set
y_test_pred_proba_rf = rf_best_model.predict_proba(X_test_scaled_2)
# Calculate precision and recall for various thresholds
precision_rf_test, recall_rf_test, thresholds_rf_test = precision_recall_curve(y_test_2_bin.ravel(), y_test_pred_proba_rf.ravel())
# Calculate AUC for test set
auc_rf_test = auc(recall_rf_test, precision_rf_test)
# Plot the precision-recall curve for test set
plt.figure(figsize=(8, 6))
plt.plot(recall_rf_test, precision_rf_test, color='green', label=f'Random Forest (Test) - AUC:{auc_rf_test:.4f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - RandomForestClassifier (Test)')
plt.legend()
plt.grid(True)
plt.show()

#ROC Curve on validation and test set of Model
def evaluate_model_with_roc(model, X, y, set_name="Set"):
    lb = LabelBinarizer()
    y_bin = lb.fit_transform(y)
    y_pred_prob = model.predict_proba(X)
    # Compute ROC curve and ROC area for each class
    n_classes = y_bin.shape[1]
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    # Plot ROC curve
    plt.figure(figsize=(8, 6))
    for i in range(n_classes):
        plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {set_name}')
    plt.legend(loc='lower right')
    plt.show()

# Evaluate on Validation Set with ROC Curve
print("Model Performance on Validation set - RandomForestClassifier:")
evaluate_model_with_roc(rf_best_model, X_valid_scaled_2, y_valid_2, set_name="Validation Set")
# Evaluate on Test Set with ROC Curve
print("Model Performance on Test set - RandomForestClassifier:")
evaluate_model_with_roc(rf_best_model, X_test_scaled_2, y_test_2, set_name="Test Set")

# Cross-Validation stability of Model 2
# Cross-validation scores with the best model
cv_scores_rf = cross_val_score(rf_best_model, X_train_scaled_2, y_train_2, scoring='f1_weighted', cv=StratifiedKFold(n_splits=5))
print("Cross-Validation Scores with Best Model of Random Forest Classification:")
print(cv_scores_rf)
print(f"Average F1 Weighted Score: {cv_scores_rf.mean():.4f}")
print(f"Standard Deviation of F1 Weighted Scores: {cv_scores_rf.std():.4f}")
# Individual cross-validation scores
for i, score in enumerate(cv_scores_rf):
    print(f"Fold {i + 1}: {score:.4f}")
# Train the model on the entire training set
rf_best_model.fit(X_train_scaled_2, y_train_2)

# Overfitting Analysis of Model B
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None,
    train_sizes=np.linspace(.1, 1.0, 5)):
    plt.figure()
    plt.title(title)

    if ylim is not None:
        plt.ylim(*ylim)

    plt.xlabel("Training examples")
    plt.ylabel("F1 Score")

    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,
    train_sizes=train_sizes, scoring='f1_weighted')

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std,
                        alpha=0.1,color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                        test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",label="Cross-validation score")
    plt.legend(loc="best")

      # Print calculations
    print(f"Train Sizes: {train_sizes}")
    print(f"Training Scores Mean: {train_scores_mean}")
    print(f"Training Scores Std: {train_scores_std}")
    print(f"Test Scores Mean: {test_scores_mean}")
    print(f"Test Scores Std: {test_scores_std}")
    return plt
    # Plot learning curves
plot_learning_curve(rf_best_model, "Model Learning Curve - Random Forest",X_train_scaled_2, y_train_2, cv=StratifiedKFold(n_splits=5), n_jobs=-1)
plt.show()

# Optimal PSP prediction for each exiting transaction
# Extract 'PSP' column for comparison
psp_column = df_2['PSP']
# Drop 'PSP' and 'success' columns from the features
X_predict = df_2.drop(['PSP', 'success'], axis=1)
# Define numeric features and apply StandardScaler
numeric_features = ['amount', '3D_secured', 'transaction_fee', 'day_of_week', 'minute_of_day',
                    'payment_attempts', 'success_probabilities']
numeric_transformer = StandardScaler()
# Define categorical features and apply OneHotEncoder
categorical_features = ['country', 'card']
categorical_transformer = OneHotEncoder()
# Create a ColumnTransformer for preprocessing
preprocessor = ColumnTransformer(
transformers=[('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, categorical_features)])
# Transform the features using the preprocessor
X_predict_transformed = preprocessor.fit_transform(X_predict)
# Predict the 'PSP' values using the best Random Forest model
df_2['Predicted_PSP'] = rf_best_model.predict(X_predict_transformed)
# Restore the original 'PSP' values for comparison
df_2['PSP'] = psp_column
# Print the DataFrame with predicted 'PSP' values
print(df_2)

def choose_best_psp(row):
    if pd.Series(row['Predicted_PSP']).nunique() == 1:
    # No tie, return the predicted PSP
        return row['Predicted_PSP']
    else:
        # There is a tie, choose the most cost-effective PSP based on fee comparison
        min_fee_psp = row.loc[row['transaction_fee'].idxmin()]['Predicted_PSP']
        # Filter rows with tied predictions
        tied_rows = row[row['Predicted_PSP'].duplicated(keep=False)]
    # Check if the row with the minimum fee is part of the tied rows
    if min_fee_psp in tied_rows['Predicted_PSP'].values:
        return min_fee_psp
    else:
        # If not, choose the PSP with the lowest transaction fee
        return tied_rows.loc[tied_rows['transaction_fee'].idxmin()]['Predicted_PSP']
# Apply the function to each row in df_2
df_2['Best_PSP'] = df_2.apply(choose_best_psp, axis=1)
# Display the updated DataFrame with the 'Best_PSP' column
print(df_2)

# Count occurrences of Predicted_PSP
predicted_psp_counts = df_2['Predicted_PSP'].value_counts()

# Count occurrences of Best_PSP
best_psp_counts = df_2['Best_PSP'].value_counts()

# Combine the counts into a DataFrame
matching_counts = pd.DataFrame({
'Predicted_PSP_Count': predicted_psp_counts,
'Best_PSP_Count': best_psp_counts
})

# Plot the count
plt.figure(figsize=(10, 5))
plt.bar(matching_counts.index, matching_counts['Predicted_PSP_Count'], label='Predicted_PSP')
plt.bar(matching_counts.index, matching_counts['Best_PSP_Count'], label='Best_PSP', bottom=matching_counts['Predicted_PSP_Count'])
plt.xlabel('PSP')
plt.ylabel('Count')
plt.title('Count of Predicted_PSP and Best_PSP')
plt.legend()
plt.show()